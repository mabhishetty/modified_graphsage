GraphSAGE first model. 1:1 graph prediction.
Begin by prepping graphs...
Number of rows in training features: 16254
Number of rows in test features: 16254
Number of rows in training labels: 16254
Number of rows in test labels: 16254
Is the ordering of training the same?: True
Is the ordering of testing the same?: True
Are the node lists the same? We have already checked each feat against its label one. Checking across train and test now...
True
Are these sorted too?
True
Features read in. Reading in graphs now...
The number of nodes in the training graph is: 19302
The number of nodes in the test list is: 19342
Iterations of this: 6
Are degree > 0 lists the same?
True
Are they sorted (degree > 0 lists?)
True
Length of degree > 0 lists is: 6446
The number of nodes in the composition is: 12892
The concepts, in order are: ['lockdown', 'mask', 'covid', 'stimulus', 'china', 'vaccine']
The concept of choice is: covid
This is at index: 2 in the list
Shape of updated feature array (train + test): (12892, 13)
Shape of updated label array (train + test): (12892, 2)
Are the final feature orderings the same?
True
Graphs prepped. Now running model...
Loading Reddit data now...
The mapping has 12892 keys
Double-checking that we have the same nodes from the graph as in the features/labels.
Is this true? True
Done loading Reddit data. Continuing with model...
Updated feature shape: (12892, 12)
Updated label shape: (12892,)
The number of training nodes we have - which should equal training numbers from earlier - is: 6446
The number of test and val nodes we have is: 6446
0 2.148249864578247
1 411.07122802734375
2 0.6931461095809937
3 0.6931461095809937
4 0.6938498020172119
5 0.6931461095809937
6 0.6931461095809937
7 0.6931461095809937
8 0.6931461095809937
9 0.6931461095809937
10 0.6931461095809937
11 0.6931461095809937
12 0.6931461095809937
13 0.6931461095809937
14 0.6931461095809937
15 0.6931461095809937
16 0.6931461095809937
17 0.6931461095809937
18 0.6931461095809937
19 0.6931461095809937
20 0.6931461095809937
21 0.6931461095809937
22 0.6931461095809937
23 0.6931461095809937
24 0.6931461095809937
25 0.6931461095809937
26 0.6931461095809937
27 0.6931461095809937
28 0.6931461095809937
29 0.6935961246490479
30 0.6931461095809937
31 0.6931461095809937
32 0.6931461095809937
33 0.6931461095809937
34 0.6933696866035461
35 0.6931461095809937
36 0.693166196346283
37 0.6931461095809937
38 0.6931461095809937
39 0.6931461095809937
40 0.6931461095809937
41 0.6931461095809937
42 0.6931461095809937
43 0.6931461095809937
44 0.6931461095809937
45 0.6931461095809937
46 0.6931461095809937
47 0.6931461095809937
48 0.6931461095809937
49 0.6931461095809937
50 0.6931461095809937
51 0.6931461095809937
52 0.6931461095809937
53 0.6931461095809937
54 0.6931461095809937
55 0.6931461095809937
56 0.6931461095809937
57 0.6931461095809937
58 0.6931461095809937
59 0.6931461095809937
60 0.6931461095809937
61 0.6931461095809937
62 0.6931461095809937
63 0.6931461095809937
64 0.6931461095809937
65 0.6931461095809937
66 0.6931461095809937
67 0.6931461095809937
68 0.6931461095809937
69 0.6931461095809937
70 0.6931461095809937
71 0.6931461095809937
72 0.6931461095809937
73 0.6931461095809937
74 0.6931461095809937
75 0.6931461095809937
76 0.6931461095809937
77 0.6931461095809937
78 0.6931461095809937
79 0.6931461095809937
80 0.6931461095809937
81 0.6931461095809937
82 0.6931461095809937
83 0.6931461095809937
84 0.6931461095809937
85 0.6931461095809937
86 0.6931461095809937
87 0.6931461095809937
88 0.6931461095809937
89 0.6931461095809937
90 0.6931461095809937
91 0.6931461095809937
92 0.6931461095809937
93 0.6931461095809937
94 0.6931461095809937
95 0.6931461095809937
96 0.6931461095809937
97 0.6931461095809937
98 0.6931461095809937
99 0.6931461095809937
Validation F1: 0.18054837040869115
Average batch time: 0.0074271059036254885
